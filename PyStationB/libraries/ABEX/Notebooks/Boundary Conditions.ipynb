{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GPs with boundary conditions\n",
    "\n",
    "In the paper entitled '' (https://export.arxiv.org/pdf/2002.00818), the author claims that a GP can be constrained to match boundary conditions. Consider a GP prior with covariance kernel\n",
    "$$k_F(x,y) = \\exp\\left(-\\frac{1}{2}(x-y)^2\\right)$$\n",
    "\n",
    "Try and match the boundary conditions:\n",
    "$$f(0) = f'(0) = f(1) = f'(1) = 0$$\n",
    "\n",
    "The posterior will be a GP with covariance equal to:\n",
    "$$\\exp\\left(-\\frac{1}{2}(x-y)^2\\right) - \\frac{\\exp\\left(-\\frac{1}{2}(x^2+y^2)\\right)}{e^{-2} + 3e^{-1} + 1} \\cdot \\left( (xy+1) + (xy-x-y+2)e^{x+y-1} + (-2xy + x+y-1)(e^{x+y-2}+e^{-1}) + (xy-y+1)e^{y-2} + (xy-x+1)e^{x-2} + (y-x-2)e^{y-1} + (x-y-2)e^{x-1}\\right)$$\n",
    "\n",
    "This notebook compares the constrained and unconstrained kernels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import GPy\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "sample_size = 5\r\n",
    "X = np.random.uniform(0, 1., (sample_size, 1))\r\n",
    "Y = np.sin(X) + np.random.randn(sample_size, 1)*0.1\r\n",
    "testX = np.linspace(0, 1, 100).reshape(-1, 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plotSamples(testY, simY, simMse, ax):\r\n",
    "    testY = testY.squeeze()\r\n",
    "    simY = simY.squeeze()\r\n",
    "    simMse = simMse.squeeze()\r\n",
    "\r\n",
    "    ax.plot(testX.squeeze(), testY, lw=0.2, c='k')\r\n",
    "    ax.plot(X, Y, 'ok', markersize=5)\r\n",
    "\r\n",
    "    ax.fill_between(testX.squeeze(), simY - 3*simMse**0.5, simY+3*simMse**0.5, alpha=0.1)\r\n",
    "    ax.set_xlabel('Input')\r\n",
    "    ax.set_ylabel('Output')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unconstrained case"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kU = GPy.kern.RBF(1, variance=1, lengthscale=1.)\r\n",
    "mU = GPy.models.GPRegression(X, Y, kU, noise_var=0.1)\r\n",
    "priorTestY = mU.posterior_samples_f(testX, full_cov=True, size=10)\r\n",
    "priorSimY, priorSimMse = mU.predict(testX)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the kernel function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n = 101\r\n",
    "xs = np.linspace(0, 1, n)[:,np.newaxis]\r\n",
    "KU = np.array([kU.K(x[np.newaxis,:], xs)[0] for x in xs])\r\n",
    "\r\n",
    "ph0 = plt.pcolormesh(xs.T, xs, KU)\r\n",
    "plt.title('Unconstrained RBF')\r\n",
    "plt.colorbar(ph0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constrained case"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def K(x, y=None):\r\n",
    "    if y is None: y = x\r\n",
    "    bb = (x*y+1) + (x*y-x-y+2)*np.exp(x+y-1) + (x+y-1-2*x*y)*(np.exp(x+y-2)+np.exp(-1)) + (x*y-y+1)*np.exp(y-2) + (x*y-x+1)*np.exp(x-2) + (y-x-2)*np.exp(y-1) + (x-y-2)*np.exp(x-1)\r\n",
    "    k = np.exp(-0.5*(x-y)**2.0) - np.exp(-0.5*(x**2.0 + y**2.0)) / (np.exp(-2) - 3*np.exp(-1) + 1) * bb\r\n",
    "    return k\r\n",
    "\r\n",
    "KC = [[K(x,y)[0] for x in xs] for y in xs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.pcolormesh(xs.T, xs, KC)\r\n",
    "plt.title('Constrained RBF')\r\n",
    "plt.colorbar()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the unconstrained model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mU.optimize()\r\n",
    "posteriorTestY = mU.posterior_samples_f(testX, full_cov=True, size=10)\r\n",
    "postSimY, postSimMse = mU.predict(testX)\r\n",
    "\r\n",
    "f, axs = plt.subplots(1, 2, sharey=True, figsize=(10,5))\r\n",
    "plotSamples(priorTestY, priorSimY, priorSimMse, axs[0])\r\n",
    "plotSamples(posteriorTestY, postSimY, postSimMse, axs[1])\r\n",
    "sns.despine()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GPy examples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combine normal and derivative observations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_gp_vs_real(m, x, yreal, size_inputs, title, fixed_input=1, xlim=[0,11], ylim=[-1.5,3]):\r\n",
    "    fig, ax = plt.subplots()\r\n",
    "    ax.set_title(title)\r\n",
    "    plt.plot(x, yreal, \"r\", label='Real function')\r\n",
    "    rows = slice(0, size_inputs[0]) if fixed_input == 0 else slice(size_inputs[0], size_inputs[0]+size_inputs[1])\r\n",
    "    m.plot(fixed_inputs=[(1, fixed_input)], which_data_rows=rows, xlim=xlim, ylim=ylim, ax=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f = lambda x: np.sin(x)+0.1*(x-2.)**2-0.005*x**3\r\n",
    "fd = lambda x: np.cos(x)+0.2*(x-2.)-0.015*x**2\r\n",
    "\r\n",
    "N = 10 # Number of observations\r\n",
    "Npred = 100 # Number of prediction points\r\n",
    "sigma = 0.2 # Noise of observations\r\n",
    "sigma_der = 1e-3 # Noise of derivative observations\r\n",
    "x = np.array([np.linspace(1,10,N)]).T\r\n",
    "y = f(x) + np.array(sigma*np.random.normal(0,1,(N,1)))\r\n",
    "\r\n",
    "#     M = 10 # Number of derivative observations\r\n",
    "#     xd = np.array([np.linspace(2,8,M)]).T\r\n",
    "#     yd = fd(xd) + np.array(sigma_der*np.random.normal(0,1,(M,1)))\r\n",
    "\r\n",
    "# Specify derivatives at end-points\r\n",
    "M = 2\r\n",
    "xd = np.atleast_2d([0, 11]).T\r\n",
    "yd = np.atleast_2d([0, 0]).T\r\n",
    "\r\n",
    "xpred = np.array([np.linspace(0,11,Npred)]).T\r\n",
    "ypred_true = f(xpred)\r\n",
    "ydpred_true = fd(xpred)\r\n",
    "\r\n",
    "# squared exponential kernel:\r\n",
    "try:\r\n",
    "    se = GPy.kern.RBF(input_dim = 1, lengthscale=1.5, variance=0.2)\r\n",
    "    # We need to generate separate kernel for the derivative observations and give the created kernel as an input:\r\n",
    "    se_der = GPy.kern.DiffKern(se, 0)\r\n",
    "except:\r\n",
    "    se = GPy.kern.RBF(input_dim = 1, lengthscale=1.5, variance=0.2)\r\n",
    "    # We need to generate separate kernel for the derivative observations and give the created kernel as an input:\r\n",
    "    se_der = GPy.kern.DiffKern(se, 0)\r\n",
    "\r\n",
    "#Then\r\n",
    "gauss = GPy.likelihoods.Gaussian(variance=sigma**2)\r\n",
    "gauss_der = GPy.likelihoods.Gaussian(variance=sigma_der**2)\r\n",
    "\r\n",
    "# Then create the model, we give everything in lists, the order of the inputs indicates the order of the outputs\r\n",
    "# Now we have the regular observations first and derivative observations second, meaning that the kernels and\r\n",
    "# the likelihoods must follow the same order. Crosscovariances are automatically taken car of\r\n",
    "m = GPy.models.MultioutputGP(X_list=[x, xd], Y_list=[y, yd], kernel_list=[se, se_der], likelihood_list = [gauss, gauss_der])\r\n",
    "m.optimize(messages=0, ipython_notebook=False)\r\n",
    "\r\n",
    "#Plot the model, the syntax is same as for multioutput models:\r\n",
    "plot_gp_vs_real(m, xpred, ydpred_true, [x.shape[0], xd.shape[0]], title='Latent function derivatives', fixed_input=1, xlim=[0,11], ylim=[-1.5,3])\r\n",
    "plot_gp_vs_real(m, xpred, ypred_true, [x.shape[0], xd.shape[0]], title='Latent function', fixed_input=0, xlim=[0,11], ylim=[-1.5,3])\r\n",
    "\r\n",
    "#making predictions for the values:\r\n",
    "mu, var = m.predict_noiseless(Xnew=[xpred, np.empty((0,1))])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fixed end-points using a Multitask GP with different likelihood functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "N = 10 # Number of observations\r\n",
    "Npred = 100 # Number of prediction points\r\n",
    "sigma = 0.25 # Noise of observations\r\n",
    "sigma_0 = 1e-3 # Noise of zero observations\r\n",
    "xlow = 0\r\n",
    "xhigh = 10\r\n",
    "x = np.array([np.linspace(xlow,xhigh,N)]).T\r\n",
    "y = f(x) + np.array(sigma*np.random.normal(0,1,(N,1)))\r\n",
    "\r\n",
    "M = 2\r\n",
    "dx = 5\r\n",
    "x0 = np.atleast_2d([xlow-dx, xhigh+dx]).T\r\n",
    "y0 = np.atleast_2d([0, 0]).T\r\n",
    "\r\n",
    "xpred = np.array([np.linspace(xlow-dx,xhigh+dx,Npred)]).T\r\n",
    "ypred_true = f(xpred)\r\n",
    "\r\n",
    "# squared exponential kernel:\r\n",
    "try:\r\n",
    "    se = GPy.kern.RBF(input_dim = 1, lengthscale=1.5, variance=0.2)\r\n",
    "except:\r\n",
    "    se = GPy.kern.RBF(input_dim = 1, lengthscale=1.5, variance=0.2)\r\n",
    "\r\n",
    "# Likelihoods for each task\r\n",
    "gauss = GPy.likelihoods.Gaussian(variance=sigma**2)\r\n",
    "gauss_0 = GPy.likelihoods.Gaussian(variance=sigma_0**2)\r\n",
    "\r\n",
    "# Create the model, we give everything in lists, the order of the inputs indicates the order of the outputs\r\n",
    "# Now we have the regular observations first and derivative observations second, meaning that the kernels and\r\n",
    "# the likelihoods must follow the same order. Crosscovariances are automatically taken car of\r\n",
    "m = GPy.models.MultioutputGP(X_list=[x, x0], Y_list=[y, y0], kernel_list=[se, se], likelihood_list = [gauss, gauss_0])\r\n",
    "m.optimize(messages=0, ipython_notebook=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot\r\n",
    "ylims = [-1.5,3]\r\n",
    "fig, ax = plt.subplots(figsize=(8,5))\r\n",
    "ax.set_title('Latent function with fixed end-points')\r\n",
    "ax.plot(xpred, ypred_true, 'k', label='Real function')\r\n",
    "\r\n",
    "ypred_mean, ypred_var = m.predict([xpred])\r\n",
    "ypred_std = np.sqrt(ypred_var)\r\n",
    "ax.fill_between(xpred.squeeze(), (ypred_mean - 1.96*ypred_std).squeeze(), (ypred_mean + 1.96*ypred_std).squeeze(), color='r', alpha=0.1, label='Confidence')\r\n",
    "ax.plot(xpred, ypred_mean, 'r', label='Mean')\r\n",
    "ax.plot(x, y, 'kx', label='Data')\r\n",
    "ax.set_ylim(ylims)\r\n",
    "\r\n",
    "ax.plot(x0, y0, 'ro', label='Fixed end-points')\r\n",
    "ax.legend()\r\n",
    "sns.despine()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fixed end-points using MixedNoise likelihood"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Squared exponential kernel:\r\n",
    "try:\r\n",
    "    se = GPy.kern.RBF(input_dim = 1, lengthscale=1.5, variance=0.2)\r\n",
    "except:\r\n",
    "    se = GPy.kern.RBF(input_dim = 1, lengthscale=1.5, variance=0.2)\r\n",
    "\r\n",
    "# MixedNoise Likelihood\r\n",
    "gauss = GPy.likelihoods.Gaussian(variance=sigma**2)\r\n",
    "gauss_0 = GPy.likelihoods.Gaussian(variance=sigma_0**2)\r\n",
    "mixed = GPy.likelihoods.MixedNoise([gauss, gauss_0])\r\n",
    "\r\n",
    "# Create the model, we give everything in lists, the order of the inputs indicates the order of the outputs\r\n",
    "# Now we have the regular observations first and derivative observations second, meaning that the kernels and\r\n",
    "# the likelihoods must follow the same order. Crosscovariances are automatically taken car of\r\n",
    "xc = np.append(x, x0, axis=0)\r\n",
    "yc = np.append(y, y0, axis=0)\r\n",
    "ids = np.append(np.zeros((N,1), dtype=int), np.ones((M,1), dtype=int), axis=0)\r\n",
    "Y_metadata = {'output_index':ids}\r\n",
    "m = GPy.core.GP(xc, yc, se, likelihood=mixed, Y_metadata=Y_metadata)\r\n",
    "m.optimize(messages=0, ipython_notebook=False)\r\n",
    "\r\n",
    "# Plot\r\n",
    "fig, ax = plt.subplots(figsize=(8,5))\r\n",
    "ax.set_title('Latent function with fixed end-points')\r\n",
    "ax.plot(xpred, ypred_true, 'k', label='Real function')\r\n",
    "#m.plot(fixed_inputs=[(1, 0)], which_data_rows=slice(0, x.shape[0]), xlim=[-dx,10+dx], ylim=[-1.5,3], ax=ax)\r\n",
    "ypred_mean, ypred_var = m.predict(xpred, Y_metadata={'output_index':np.zeros_like(xpred, dtype=int)})\r\n",
    "ypred_std = np.sqrt(ypred_var)\r\n",
    "ax.fill_between(xpred.squeeze(), (ypred_mean - 1.96*ypred_std).squeeze(), (ypred_mean + 1.96*ypred_std).squeeze(), \r\n",
    "                color='r', alpha=0.1, label='Confidence')\r\n",
    "ax.plot(xpred, ypred_mean, 'r-', label='Mean')\r\n",
    "ax.plot(x, y, 'kx', label='Data')\r\n",
    "ax.set_ylim(ylims)\r\n",
    "ax.plot(x0, y0, 'ro', label='Fixed end-points')\r\n",
    "ax.legend()\r\n",
    "\r\n",
    "sns.despine()\r\n",
    "\r\n",
    "m"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (abex)",
   "language": "python",
   "name": "abex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}