{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Extend kernel with additive noise for file ID\n",
    "\n",
    "This notebook explores how to model the observation that experiments performed on different days, potentially prepared by different operators, lead to systematic differences in the measurable output\n",
    "\n",
    "## Approach\n",
    "\n",
    "The general idea is to consider a file-specific additive noise variable. To start thinking about this, consider data in file $j$ which are described by \n",
    "$$y_j = f_1(x_j) + \\delta_j + \\epsilon$$\n",
    "where \n",
    "- $f_1\\sim\\mathcal{GP}(0, k_{x,x}(\\theta_1))$ is the underlying process with covariance parameters $\\theta_1$, \n",
    "- $\\delta_j\\sim\\mathcal{N}(0,\\sigma_f^2)$ is file-specific noise, and \n",
    "- $\\epsilon\\sim\\mathcal{N}(0, \\sigma^2)$ is the measurement noise.\n",
    "\n",
    "One way to handle this conveniently is to consider a new input vector $u$ which is a one-hot encoding of the file ID $j$. Then we can write $\\delta_j = f_2(u)$, with \n",
    "$$f_2\\sim\\mathcal{GP}(0, k_{u,u}(\\theta_2))$$\n",
    "where $\\theta_2$ parameterises the covariance function of the file-specific noise.\n",
    "\n",
    "Overall, we end up with a standard GP structure $y = f(x,u) + \\epsilon$ where \n",
    "$$f(x,u) = f_1(x) + f_2(u)$$\n",
    "\n",
    "### Log-scale\n",
    "\n",
    "The problem with the approach above is that we know from experience that the output data are described better in log-space. Typically, we model as \n",
    "$$\\log_{10}(y_j) = f_1(x_j) + \\epsilon$$\n",
    "But this is incompatible with the approach above, as adding a second kernel $f_2$ in the log-space is equivalent to multiplication in the real space. What we really want is\n",
    "$$y_j = \\exp\\{f_1(x_j)\\} + f_2(u) + \\epsilon$$\n",
    "\n",
    "## Load dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import argparse\r\n",
    "import pandas as pd\r\n",
    "from pathlib import Path\r\n",
    "import GPy\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "import sys\r\n",
    "sys.path.insert( 0, '../')\r\n",
    "sys.path.insert( 0, '../abex')\r\n",
    "sys.path.insert( 0, '../pyBCKG')\r\n",
    "sys.path.insert( 0, '../emukit')\r\n",
    "\r\n",
    "import settings\r\n",
    "from data import Dataset, onehot\r\n",
    "import optim\r\n",
    "import plotting\r\n",
    "\r\n",
    "from bayesopt import BayesOptModel, flatten_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fake data example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def real_function(x):\r\n",
    "    return x[:,0]*x[:,1] - 0.5*x[:,0] + 0.3*x[:,1] + 1.5\r\n",
    "\r\n",
    "sigf = 1\r\n",
    "sigy = 0.02\r\n",
    "\r\n",
    "ns = 40\r\n",
    "nf = 7\r\n",
    "js = sigf*np.random.rand(nf,1)\r\n",
    "xs = [np.random.rand(ns,2) for f in range(nf)]\r\n",
    "X = np.concatenate(xs, axis=0)\r\n",
    "print(X.shape)\r\n",
    "F = np.vstack([f * np.ones((ns,1)) for f in range(nf)])\r\n",
    "y = list(map(real_function, xs))\r\n",
    "ys = np.concatenate([yi + js[f] for f,yi in enumerate(y)], axis=0)[:,np.newaxis]\r\n",
    "print(ys.shape)\r\n",
    "y = np.reshape(y, (ns*nf,1))\r\n",
    "print(y.shape)\r\n",
    "Y = ys + sigy*np.random.randn(ns*nf,1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "from mpl_toolkits.mplot3d import Axes3D\r\n",
    "fig = plt.figure()\r\n",
    "ax1 = fig.add_subplot(131, projection='3d')\r\n",
    "ax1.scatter3D(X[:,0],X[:,1],y[:,0])\r\n",
    "ax2 = fig.add_subplot(132)\r\n",
    "ax2.scatter(y[:,0], ys[:,0])\r\n",
    "ax3 = fig.add_subplot(133)\r\n",
    "ax3.scatter(y[:,0], Y[:,0])\r\n",
    "\r\n",
    "plt.tight_layout()\r\n",
    "sns.despine()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame(np.hstack([X, F, y, ys, Y]), columns=['x0','x1','f','y (noise-free)','y (+file noise)','y (both noise)'])\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f, axs = plt.subplots(1, 2, figsize=(9,4))\r\n",
    "sns.scatterplot(x='x0', y='y (both noise)', hue='f', data=df, ax=axs[0])\r\n",
    "sns.scatterplot(x='x1', y='y (both noise)', hue='f', data=df, ax=axs[1])\r\n",
    "sns.despine()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train model without file information"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "k0 = GPy.kern.Matern52(2)\r\n",
    "model0 = GPy.models.GPRegression(X, Y, k0)\r\n",
    "model0.optimize()\r\n",
    "model0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "m0_mean, m0_var = model0.predict(X)\r\n",
    "rs = np.corrcoef(Y.squeeze(), m0_mean.squeeze())\r\n",
    "r = rs[1,0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f, axs = plt.subplots(2, 2, figsize=(10,10))\r\n",
    "\r\n",
    "model0.plot(ax=axs[0,0], fixed_inputs=[(1,0.5)])\r\n",
    "axs[0,0].set_xlabel('x0')\r\n",
    "axs[0,0].set_ylabel('y')\r\n",
    "model0.plot(ax=axs[0,1], fixed_inputs=[(0,0.5)])\r\n",
    "axs[0,1].set_xlabel('x1')\r\n",
    "axs[0,1].set_ylabel('y')\r\n",
    "model0.plot(ax=axs[1,0])\r\n",
    "axs[1,0].set_xlabel('x0')\r\n",
    "axs[1,0].set_ylabel('x1')\r\n",
    "\r\n",
    "axs[1,1].errorbar(Y.squeeze(), m0_mean.squeeze(), np.sqrt(m0_var.squeeze()), fmt='.')\r\n",
    "axs[1,1].set_title('r = %1.2f' % r)\r\n",
    "axs[1,1].set_xlabel('Data')\r\n",
    "axs[1,1].set_ylabel('Model')\r\n",
    "\r\n",
    "sns.despine()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train model with file information"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "U = np.concatenate([np.tile(onehot(nf,f),(ns,1)) for f in range(nf)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "XU = np.hstack([X,U])\r\n",
    "for f in range(nf):\r\n",
    "    plt.plot(range(ns*nf), U[:,f], '.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "k1 = GPy.kern.Matern52(nf+2, active_dims=np.append(np.ones(2), np.zeros(nf)))\r\n",
    "#k2 = GPy.kern.RBF(nf, active_dims=range(2,nf+2))\r\n",
    "#k2 = GPy.kern.RBF(nf, active_dims=range(2,nf+2), ARD=True)\r\n",
    "k2 = GPy.kern.Linear(nf, active_dims=range(2,nf+2))\r\n",
    "combined_kernel = k1 + k2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model1 = GPy.models.GPRegression(XU, Y, combined_kernel)\r\n",
    "model1.optimize()\r\n",
    "model1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "m1_mean, m1_var = model1.predict(XU)\r\n",
    "rs1 = np.corrcoef(Y.squeeze(), m1_mean.squeeze())\r\n",
    "r1 = rs1[1,0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f, axs = plt.subplots(2, 2, figsize=(10,10))\r\n",
    "\r\n",
    "no_noise = [(i,0) for i in range(2,nf+2)]\r\n",
    "model1.plot(ax=axs[0,0], fixed_inputs=[(1,0.5)]+no_noise)\r\n",
    "axs[0,0].set_xlabel('x0')\r\n",
    "axs[0,0].set_ylabel('y')\r\n",
    "model1.plot(ax=axs[0,1], fixed_inputs=[(0,0.5)]+no_noise)\r\n",
    "axs[0,1].set_xlabel('x1')\r\n",
    "axs[0,1].set_ylabel('y')\r\n",
    "model1.plot(ax=axs[1,0], fixed_inputs=no_noise)\r\n",
    "axs[1,0].set_xlabel('x0')\r\n",
    "axs[1,0].set_ylabel('x1')\r\n",
    "\r\n",
    "axs[1,1].errorbar(Y.squeeze(), m1_mean.squeeze(), np.sqrt(m1_var.squeeze()), fmt='.')\r\n",
    "axs[1,1].set_title('r = %1.2f' % r1)\r\n",
    "axs[1,1].set_xlabel('Data')\r\n",
    "axs[1,1].set_ylabel('Model')\r\n",
    "\r\n",
    "sns.despine()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparison of the underlying real function and the inferred function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ng = 21\r\n",
    "xg = np.linspace(0,1,ng)\r\n",
    "x1, x2 = np.meshgrid(xg, xg)\r\n",
    "yreal = [real_function(np.hstack([xi*np.ones((ng,1)),xg[:,np.newaxis]])) for xi in xg]\r\n",
    "ygp0_mean, ygp0_var = zip(*[model0.predict(np.hstack([xi*np.ones((ng,1)),xg[:,np.newaxis]])) for xi in xg])\r\n",
    "ygp1_mean, ygp1_var = zip(*[model1.predict(np.hstack([xi*np.ones((ng,1)),xg[:,np.newaxis],np.zeros((ng,nf))])) for xi in xg])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ygp0 = np.array(ygp0_mean).squeeze()\r\n",
    "ygp1 = np.array(ygp1_mean).squeeze()\r\n",
    "\r\n",
    "plt.pcolormesh(x1, x2, yreal, shading='auto', vmin=1, vmax=2.5)\r\n",
    "plt.colorbar()\r\n",
    "\r\n",
    "# Estimate the bias from the file noise\r\n",
    "bias = js.mean()\r\n",
    "\r\n",
    "f, axs = plt.subplots(1,3,figsize=(8,4))\r\n",
    "p0 = axs[0].pcolormesh(x1, x2, np.abs(ygp0-yreal-bias), shading='auto', vmin=0.0, vmax=0.2, cmap='jet')\r\n",
    "axs[0].set_title('Simple model: $y = f_1(x)$')\r\n",
    "axs[1].pcolormesh(x1, x2, np.abs(ygp1-yreal-bias), shading='auto', vmin=0.0, vmax=0.2, cmap='jet')\r\n",
    "axs[1].set_title('File-noise model: $y = f_1(x) + f_2(u)$')\r\n",
    "axs[0].set_position([0.1, 0.1, 0.4, 0.8])\r\n",
    "axs[1].set_position([0.58, 0.1, 0.4, 0.8])\r\n",
    "axs[2].set_position([1, 0.1, 0.01, 0.8])\r\n",
    "plt.colorbar(p0, cax=axs[2], label = '$| f_{real} - f_1 - b |$')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ygp0 = np.sqrt(np.array(ygp0_var).squeeze())\r\n",
    "ygp1 = np.sqrt(np.array(ygp1_var).squeeze())\r\n",
    "\r\n",
    "f, axs = plt.subplots(1,4,figsize=(9,4))\r\n",
    "p0 = axs[0].pcolormesh(x1, x2, ygp0, shading='auto')\r\n",
    "axs[0].set_title('Simple model: $y = f_1(x)$')\r\n",
    "p1 = axs[2].pcolormesh(x1, x2, ygp1, shading='auto')\r\n",
    "axs[2].set_title('File-noise model: $y = f_1(x) + f_2(u)$')\r\n",
    "axs[0].set_position([0., 0.1, 0.35, 0.8])\r\n",
    "axs[1].set_position([0.38, 0.1, 0.01, 0.8])\r\n",
    "plt.colorbar(p0, cax=axs[1])\r\n",
    "axs[2].set_position([0.5, 0.1, 0.35, 0.8])\r\n",
    "axs[3].set_position([0.88, 0.1, 0.01, 0.8])\r\n",
    "plt.colorbar(p1, cax=axs[3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "It looks like the additive file noise model leads to a better reconstruction of the original model, but with a constant bias. Ignoring the file noise leads to a model with slightly the wrong shape. \n",
    "\n",
    "The biggest effect is that including the file-noise enables much better accounting of uncertainty."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (abex)",
   "language": "python",
   "name": "abex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}